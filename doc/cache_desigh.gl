#资料来源于网络  感谢http://talkpower.info/  谢谢前辈的详细分解。

缓存服务器设计与实现(一)
Sunday 23 November 2014
这里我们nginx的cache系统为线索，来探讨一个缓存服务器的设计和相关细节，我尽量站在设计和框架的角度来分析，限于篇幅这里不再去撸代码了，相关的细节，欢迎大家一起参与讨论。
一个cache服务器中从后端取得文件之后，要么直接发送给客户端(学名叫透传)，要么缓存在本地，后续相同的请求访问到cache服务器时，就可以直接拿本地的拷贝来用了，如果可以用的话。如果本地缓存的文件被后续的请求访问到，在cache中叫做命中(即Hit)。如果本地还没有文件的缓存拷贝，那么cache服务器需要根据配置或者做解析域名，去后端获取文件，这时称为缓存miss，即未命中。关于cache服务器更多的知识，我们在分析nginx的缓存系统时再深入讨论。
nginx的存储系统分两类，一类是通过proxy_store开启的，存储方式是按照url中的文件路径，存储在本地。比如/file/2013/0001/en/test.html，那么nginx就会在指定的存储目录下依次建立各个目录和文件。另一类是通过proxy_cache开启，这种方式存储的文件不是按照url路径来组织的，而是使用一些特殊方式来管理的(这里称为自定义方式)，自定义方式就是我们要重点分析的。那么这两种方式各有什么优势呢？
按url路径存储文件的方式，程序处理起来比较简单，但是性能不行。首先有的url巨长，我们要在本地文件系统上建立如此深的目录，那么文件的打开和查找都很会很慢(回想kernel中通过路径名查找inode的过程吧)。如果使用自定义方式来处理模式，尽管也离不开文件和路径，但是它不会因url长度而产生复杂性增加和性能的降低。从某种意义上说这是一种用户态文件系统，最典型的应该算是squid中的CFS。nginx使用的方式相对简单，主要依靠url的md5值来管理，后面我们会分析。
缓存离不开从后端取内容，然后发送给客户端。具体的处理方式大家很容易就会想到，肯定是一边接收一边发送，其他的方式都太低效了，如读完再发等等。这里提一下nginx边收边发，使用的结构是ngx_event_pipe_t，它是沟通后端和客户端的媒介。由于该结构是一个通用组件，所以需要一些特殊的标记来处理涉及存储的相关功能，那么成员cacheable就担当了这份重任。
p->cacheable = u->cacheable || u->store;
即cacheable为1，则需要存储，否则不存储。那么u->cacheable跟u->store代表什么？他们分别代表前面说的两种方式，即proxy_cache和proxy_store。
(补充一些知识，nginx在取后端数据时，它的行为受proxy_buffering控制，作用是为后端的服务器启用应答缓冲。如果启用缓冲，nginx假设被代理服务器能够非常快的传递应答，并将其放入缓冲区，可以使用proxy_buffer_size和proxy_buffers设置相关参数。如果响应无法全部放入内存，则将其写入硬盘。如果禁用缓冲，从后端传来的应答将立即被传送到客户端。)
这里都是一些擦边球，我们还没有接触nginx cache功能的核心。从实现上看，在nginx upstream结构中有个成员叫cache，它的类型是ngx_shm_zone_t。如果我们开启cache功能，cache成员用来管理共享内存(为什么用到了共享内存？)，而其他方式的存储该成员都为NULL。另外有一点需要说明一下，在cache系统中一个文件通常被称为store object，即缓存对象，所以进行cache之前必然需要先创建一个store object。一个重要的问题就是如何选择创建的时机，这点大家有什么看法？首先我们需要检查一个文件是否是需要缓存，很明显GET方法请求的文件一般需要缓存，所以我们在请求处理的前期，看到了GET方法，就可以先创建一个对象。但是很多时候，即使是一个GET方法请求的文件也不能缓存，那么你过早的创建对象，不仅浪费时间也浪费了空间，到头来还要将它销毁。那么什么会影响GET请求的存储呢？那就是响应头中的Cachecontrol字段，这个字段就告诉代理或者浏览器，该文件能否被缓存。一般的cache服务器面对响应头中没有Cache-control字段的请求，默认都是要缓存的。
基于这一点的考虑，我们开发的cache服务器就是在响应头解析完成，拿到可缓存的足够证据之后，才会创建缓存对象。遗憾的是，nginx没有这么去做。
nginx在ngx_http_upstream_init_request函数中完成缓存对象的创建，这个函数处在http处理的什么阶段呢？在跟后端建立连接之前。这个地方，我个人认为不太合适。。。大家认为呢？
关于创建过程，大家可以去读函数ngx_http_upstream_cache。这里我拿我们的cache跟nginx对比来分析吧。我们的request中使用一个名叫store的成员，来跟缓存对象建立联系。nginx也差不多，它的request结构体中有个cache成员来做同样的事情。区别在于我们的store成员对应的空间在共享内存中，而nginx则是在r->pool里申请的(我们为什么这么做？)。
下一步，nginx需要根据配置来生成缓存对象的key，此处一般都是用md5来算的。这个key作为一个缓存对象在系统中的唯一标识，很多人可能担心md5碰撞的问题。这个我认为要求如果不是特别苛刻，这里完全可以接受的，而且处理也相对简单。
后面要处理的是，文件到底应该已怎样的形式存储在磁盘？ 我们拿前面用过的一个例子：/file/2013/0001/en/test.html，它对应的md5值是8ef9229f02c5672c747dc7a324d658d0，实际上nginx就用它当做文件名。这样就可以了？如果我们找一个目录来存放文件，里面都是一堆这样的文件，那么会怎样？我们知道，大多数文件系统下，都对单个目录下的文件数量有限制，所以这样简单粗暴的处理是不行的。那怎么办？nginx通过配置可以让你使用多级目录，来解决这个问题。简单来说，nginx通过levels这个指令指定目录层数(冒号分隔)和每个目录名字的字符个数，在我们的例子中，假设配置levels=1:2，意思是说使用两级目录，第一级目录名是一个字符，第二级用两个字符。但是nginx最大支持3级目录，即levels=xxx:xxx:xxx。
那么构成目录名字的字符哪来的呢？假设我们的存储目录为/cache，levels=1:2，那么对于上面的文件 就是这样存储的，从后往前，最后一个字符做一级目录，即"0"， 倒数2，3两个字符做二级目录，即"8d"：
/cache/0/8d/8ef9229f02c5672c747dc7a324d658d0
对象创建完成之后，就需要缓存对象管理结构中去了，这个ngx_http_file_cache_exists去处理的。
如果在创建这个文件时，当前目录及文件已经存在，那如何处理？大家可以去翻翻代码，看nginx怎么处理的。
讨论先告一个段落，其实现在都是一些准备工作，下次讨论后端内容到来的处理。
扩展阅读：
http://www.pagefault.info/?p=123
http://www.pagefault.info/?p=375
 

缓存服务器设计与实现(二)
Sunday 30 November 2014
我们现在讨论算是最简单的情景，即服务器还没有文件缓存，第一个需要缓存的请求的处理过程。当然需要关注的情景有很多，一个一个来吧。
在缓存服务器设计与实现(一)中讨论的都是一些准备工作，我们接下来要关注从后端机器取回数据以后进行缓存的情景。首先来探讨一个问题,以nginx为例，它是在取后端数据之前就创建了缓存对象，那么从整个系统的角度来看，创建缓存对象的过程包括在内存中建立相应的控制结构，并且在磁盘上创建实体(文件的形式)。那么我们需要关注的是这两部分都有些什么成分？先看磁盘上的文件，它应该存什么。存储实际文件内容是必然的，这就够了吗？
我们知道作为一个http缓存系统，首先它是一个完整的http服务器。所以在响应一个客户端的请求时，必须先给出一个http响应头，然后才是内容。当nginx作为一个静态http服务器工作的时候，响应头是nginx自己构造的，想怎么搞都可以。但是当它作为一个缓存服务器使用时，响应头应该尽量跟被代理的后端服务器一致，甚至严格一致。那么此时，响应头神马的就不能自己杜撰了。从这个角度上来讲把响应头头跟文件内容同时缓存起来是合适，也是必要的。你可以尝试打开一个nginx缓存好的文件，就会发现在具体内容之前确实是保存着相应的响应头的。不过在文件的最开始(响应头之前)貌似还有一些东西，关于这些神秘的东西，随着我们的讨论，都会搞清楚的。其实几乎所有的http缓存服务器都是这么做的，即将http响应头和内容都缓存在文件中。
刚才我们关注的是缓存对象在磁盘上的内容组织，下面我们再看一下内存中的控制结构。
在nginx中每个文件都在内存中都有相应的控制结构，称为一个node。这个结构是在共享内存中申请和管理的，为什么用共享内存？nginx作为多进程模型，我们希望在worker A中缓存的文件对象，在worker B中相同请求到来时也能够hit该对象，要不然就太废了。当然互斥也是不可避免的。
在nginx具体实现中，这个node是结构体ngx_http_file_cache_node_t。关于这个结构中各个成员的说明，可以参考这里，这里就不多说了。当一个node首次创建之后，需要放入到系统的缓存管理体系中，nginx用到的是红黑树，所有的node都被插入到树里面，然后还要放到lru队列中，作用就是在存储空间不够的时候，通过lru来删掉一些对象。我们的cache在lru方面跟nginx是一致的，但是所有node是通过hash表来管理的。
其实细节需要讨论的东西实在是太多。现在还是先转到重点上来，来看一下nginx如何将收到的后端数据写到本地磁盘文件里去。
核心思想是这样的，nginx首先会将数据写到一个临时文件中去，然后内容收完之后，再将这个临时文件rename到实际的目标文件。这是主要框架，至于如何管理临时文件，又该如何去实现，nginx有它的处理，我们自己实现一套也可以，这都是一些无关紧要的细节了。重点的问题其实是要维护我们的缓存系统中有关当前缓存文件的状态，缓存过程是OK的，该怎么处理，出现异常之后，又该如何处理。
先看当这个文件缓存ok的时候，该如何更新缓存信息，来声明文件已经缓存完毕，可供使用了。这里还是以nginx为例： 在文件获取完成之前，缓存对象的node节点结构中exists成员一直为0，当缓存正常结束时exists就会被置1。所以这个成员的含义就很明确了。这里注意的是，缓存对象的node节点信息位于共享内存中红黑树中，是唯一的。而每个request通过一个cache成员，来跟这个node发生联系。
现在有一个相同的请求过来，首先要做的事情就是查找是否有已经缓存的目标文件。如果文件存在，一般需要先将文件开始部分的一些控制和管理信息读取出来，通过分析这些信息来判断该文件是否是可用的，如未过期或者完好等等。如果可用，那么剩下的工作就是发送内容了，不过在实际发送之前，一般需要先将读取出来的响应头做一些header filter的处理等等，后面的文件内容一般直接发送就好了。
先做一个阶段性的总结吧。到目前为止，我们看到的过程包括：第一个请求到来->去后端取数据缓存；同样的请求到来->发现了刚刚创建的文件->文件可用->发送这个文件。从nginx实现上看，它的ngx_http_file_cache_node_t结构中有两个成员需要注意，一个是count，表示当前正在使用这个node的请求数，另一个成员uses记录了到目前为止，这个node被访问的次数，这个值是一直累加的。
如果一个文件过期了，此时当有请求再次访问到这个文件时，该如何处理呢？这里应该提一下跟cache相关的管理进程(后面会拿出篇幅来重点讨论他们)，这其中有一个非常重要的后台程序，它跟普通的worker进程一样，是由master进程fork出来的。ps命令看到的进程title一般为：“nginx: cache manager process”，它的一个重要的作用就是监控过期，做lru等工作。我们把这个manager进程发现过期的情景称为主动发现，worker进程在处理请求时也会发现某个对象过期，这个称为被动发现。能做到这些，它的基础就在于管理缓存对象的控制结构及其信息是由master进程通过共享内存创建并初始化的，这样manager和worker在被fork之后，这些信息就是共享的了。manager进程在运作时，从lru队列的尾部开始，检查是否有文件过期。有过期的就删掉。至于为什么从lru队列的尾部检查，是因为在worker处理对象的时候，每次hit一个文件(一个正常的文件必然位于lru队列中)，该文件就从队列中删掉，然后插到队列头部去，所以越靠近尾部的对象，越是较长时间没有被访问到的，LRU的思想也在于此。当然一直以来很多人都在批评这种处理，说它不科学不严谨。是的，这点没错。网上有很多更高级更严谨的lru理论及实现，但是在实际应用中我们不能死磕理论，往往需要结合系统复杂度，实现难易等各方面的考虑。我们自己的cache，包括squid，nginx都是用的这种最简单处理方式，而事实也表明：运行表现不错。
貌似扯远了。我们现在重点关注当worker进程发现了一个对象过期时，它会如何去处理。首先一个问题就是进程如何发现一个文件时过期的？其实在一个文件的开头部分，存放了一些有关文件的控制头信息，前面已经提过了。这个头信息中有相关的变量标记这个对象保鲜时间，所以只需要读出这个变量跟当前时间比较一下就可以了。 从系统的控制结构来看，有个名叫updating的变量此时会被置1。后续的处理就很显然，就是去后端取新文件了。注意此时这个文件相关的管理结构还未被从系统里释放，所以后续的请求还是会hit，不过后面的处理还是会发现过期，这样只要一个请求在更新完成之前，对于该文件的请求，都会去后端取文件(即透传)。前面我们分析过了，nginx会先用临时文件来保存数据，完整取完之后会rename到缓存目录下去。那么当文件过期，但此时有又同一并发请求，那么最后谁去rename呢？说得这里就不得不提另外一个rename，那就是针对一个文件的首次并发请求，各个请求都是独立取源，最后也会出现同时rename的情况。呵呵，看一篇文章吧：http://www.ibm.com/developerworks/cn/linux/l-cn-fsmeta/
上面讲到nginx会出现并发取源的情况，很多公司对这块进行了定制。最常提到的是所谓取源合并。顾名思义，就是合并回源的请求。这项功能，nginx在比较新的版本里面已经支持了，使用的指令时proxy_cache_lock。官方wiki给出的说明:
syntax: proxy_cache_lock on | off;
default: proxy_cache_lock off;
context: http, server, location;
This directive appeared in version 1.1.12.
When enabled, only one request at a time will be allowed to populate a new cache element identified according to the proxy_cache_key directive by passing a request to a proxied server. Other requests of the same cache element will either wait for a response to appear in the cache, or the cache lock for this element to be released, up to the time set by the proxy_cache_lock_timeout directive.
另外一个地方就是当文件过期时，也会产生大量的并发回源量。这点nginx也做了处理，很多公司也对这块做了自己的定制。nginx通过指令proxy_cache_use_stale来控制在文件过期更新过程的回源请求量，让当一个请求在更新文件时，其他请求则暂时使用过期文件。具体配置为proxy_cache_use_stale updating;关于该指令的具体用法，大家可以去官网查阅。
有一些机制，后面再接着讨论。


缓存服务器设计与实现(三)
Sunday 30 November 2014
这里我们讨论一个比较重要功能，在之前的文章中提到过，取源合并。为什么要单独把它拿出来讨论呢？其实主要是出于个人工作的角度。之前公司里的cache需要这样一个功能，现有的squid该功能不完善，并且也不太适合我们的业务。然后我们分别在cache和nginx上加了这个功能，不过现在的nginx版本已经原生支持了。呵呵，您也许不知道我们的nginx可是0.7.x的版本，我们当时开发的时候还在想，也许不久官方能做支持，可是谁知道这个不久是多久呢。
进入正题吧，看看nginx如何去设计和实现的。
这里我们把取源合并成为fetch merged。在默认情况下，nginx是不启用它的。要想使用，可以通过一个命令开启，即proxy_cache_lock，这里再次贴出官方wiki的说明：
syntax: proxy_cache_lock on | off;
default: proxy_cache_lock off;
context: http, server, location
This directive appeared in version 1.1.12. When enabled, only one request at a time will be allowed to populate a new cache element identified according to theproxy_cache_key directive by passing a request to a proxied server. Other requests of the same cache element will either wait for a response to appear in the cache, or the cache lock for this element to be released, up to the time set by the proxy_cache_lock_timeout directive.
如果开启的话，在ngx_http_cache_t结构中有个成员叫lock，它会被置1。这个结构前面已经提过了，在一个cache中，总要有一个结构来沟通请求与其对应的缓存对象。同样的，在nginx中就是通过该结构。
让我们想想流程。对象在取源的时候，其相应的控制结构已经存在，并且在系统中也已经可见。也就是说在一个请求正在取源的过程中，如果后续的同一请求再次到来，那么必然会找到这个对象。显然此时的对象跟正常的是有区别的，在cache中我们通常用pending和ok状态来区分两者。nginx并没有通过这些状态来处理，当一个对象未缓存完整时，一个名为updating的成员会置1，同时exists成员置0。这两个成员在整个缓存过程中起到了很重要的作用，不仅仅是fetch merged。如果哪位同学要去翻阅代码，那这两个成员的运用一定要注意。
在开启fetch merged的时候，nginx会让后续的请求等待一段时间(默认是5秒)。在这段时间内，nginx每500ms去检查缓存是否正常结束(updating是否被置0)，如果结束了，那么就会在后面的处理流程中发现已经缓存好的文件，即可hit了。如果时间挺长，过了等待的最长时间，那么针对该缓存文件的fetch merged机制就会cancel，这样后续的请求就会跟普通情况下一样，继续去后端fetch文件了。当然这里的cancel处理不会影响其他文件的fetch merged。
上面说过在fetch merged中，默认的等待时间是5秒，这个时间是可以通过配置来控制的。即proxy_cache_lock_timeout，具体使用方法可以去官方wiki查看。聊到这里，各位可能看出来了。nginx这个东西做的有点鸡肋，因为它为了减轻后端的压力，却给客户端造成了延迟。特别是对于大文件取源时间长，一来你要让客户端去等，二来等待时间过去之后，又要去取源。我认为很多公司可能对nginx这个功能都不太认可，特别是CDN公司。一般公认的比较好的处理方式就是一边接收一边分发，这样子很合理。不过在处理起来有些麻烦，接下来的时间，就跟大家一起来讨论下这个方案吧。
首先需要第一个请求去后端取数据，在这期间，相同请求过来的时候，需要有种机制来发现那个正在进行取源的请求，进而来完成合并。nginx用的是rbtree，所有的缓存对象都以有一个node的形式登记在树中(关于这点，我了解的比较多的情况是用hash表，那种使用冲突链表的实现)。找到取源的对象才是第一步，后面的步骤就是要通过这个请求来做数据分发，最简单的方式就是这个request的结构中添加一个队列，需要合并的请求挂载到上面去，然后取回来的数据分发给队列上的每个请求。但是问题并不这么简单：当这个取源的请求取源结束，一般的处理是要释放该请求，那么我们需要在释放请求之前保证数据分发完，或者通过其他的机制来让该请求释放之后，能够完成后续的分发工作。对于前者，我们不得不在request结构体中增加引用计数，或者其他的计数方式。只有计数为0，才意味着分发工作，请求可以释放了。这种处理是可以的，但是如果你是在原来的系统上搞二次开发，那么你需要在暴多的地方来添加计数检查的代码，那真是梦魇。我们用的是后者，具体地，是将需要合并的请求挂载一个全局的hash表，需要合并的请求会在表中添加一个条目，我们不妨称为一个node，这个node中管理了一个队列，上面管理着同一个文件的所有合并请求。我们需要做的就是让这个node在数据完全分发完之后，才会被销毁。这点的处理不像那种request中添加计数那样，相对来说代码耦合小很多，而仅仅多了一个hash表的内存消耗。
分发动作其实很简单。首先的准备工作就是在request中添加一些成员，标记已发多少，下次需要发多少之类的。在事件方面的处理上，客户端关闭的事件，通过注册读事件处理函数就可以发现。对于写，当取源正在进行时，我们不需要设置什么处理函数，此时的发送依赖于取源请求的分发。而取源请求的分发本身依赖于读写的触发，所以当取源结束时，也就不会有事件来驱动分发了。这也好处理，当取源完成以后做最后一次分发的时候，给每个合并的请求设置新的读写事件处理函数，这样一来，后续的分发就可以通过读写事件来驱动，并且大家也有缓存完好的文件可用。当然具体实现方法有很多，但是思路一般就是这样子了。那么在这种模型下，怎么合理正确的处理异常呢？我们的处理遵循简单粗暴的原则：如果客户端关了，那么没啥好说的，直接将它从队列里踢掉就行了。如果取源的连接出现了问题，则依次关闭队列上的各个请求连接。这里其实不需要考虑什么更友好的处理方式，因为源宕机一般不能瞬间恢复，给这些合并的请求找个“后妈”，往往也于事无补。不过像nginx这种有负载均衡机制的，可以去尝试找个“后妈”。
这里提一下关于多进程取源合并的两种实现，第一种是进程内合并，另一种是进程间合并。对于进程内的，合并请求node使用的hash表是进程内的，所以同一文件会有最大进程个数数量的并发连接取源，这个并发量是完全可以接受的。如果我们将这个hash放到共享内存中，就有希望实现进程间完美的合并。但是仅仅这样就可以了吗？进程间合并是个棘手的事。首当其冲的问题就是谁负责分发？进程内合并，取源请求可以肩负起分发的重任，因为队列上的请求都是属于本进程的。如果我们将hash表及其队列放到了共享内存，分发的时候就要区分哪些是属于自己进程的。哪些不属于自己进程的呢？对于基于磁盘文件的cache，相对好处理一些。因为各个进程内的请求都可以在每次有可写事件时，去磁盘上查看是否有新内容，从而将数据发送出去。万一没有新数据的话，又要重新添加事件，这样每次epoll_wait都会报很多事件。这仅仅一种实现。
再看全内存cache，它的文件内容全部放在内存中。那么通过查看磁盘文件来发送数据的路彻底没戏了，那么内容如何分发，换句话说，除了取源请求所在的进程，其他进程中的合并请求如何被告知，何时有新内容要发送？之前我们讨论过一个方案，就是这些请求设置一些定时器，过一段时间去查看内容是否有更新，但这样跟nginx的实现类似了，由于定时器的存在，客户端增加了延迟。加事件的方式前面讨论了，也不行。所以无论磁盘缓存还是内存缓存，通知何时可分发是个关键问题，也是最棘手的问题。我们的目标就是要让其他进程异步得到通知，而不是忙等(定时器和事件在这里就是)，所以信号貌似可以：取源的进程向其他进程发送信号，告诉他们有内容更新了。收到信号的进程在信号处理函数中完成数据的分发，但是这样做会导致系统内信号满天飞，加上信号处理函数的特殊性，这种方案的可行性有待商榷。我们的程序经过调研之后还是选择了进程内的合并，目前看来完全满足需求。nginx虽然解决了进程间合并的这个问题，但是它以客户端的的延迟做代价，这点在我们的CDN业务上是难以接受的。
所以既不过多影响性能，有能实时异步的实现进程间合并，还是有很多问题要解决的。这里留着跟大家一起讨论吧。



缓存服务器设计与实现(四)
Sunday 30 November 2014
这里我们聚焦一个问题，就是缓存满的情况。一般的cache都会配置容量，无论是内存缓存还是磁盘缓存，都不能无节制的去使用他们。这里以磁盘缓存为例，如果配置的限额已用完，该如何处理呢？
对于nginx，如果你开启了cache功能，那么你通过ps命令看到这样的进程：cache manager process。其实这个进程的作用主要是在文件失效或者磁盘空间不足的时候，删除对象。那么怎么删，什么标准？
由于每个缓存对象都有一个最长有效期，过了这个时间对象就认为无效了，需要删掉。在nginx的缓存系统中有个lru队列，里面的元素其实是对象在rbtree中的一个node。每次hit一个对象，对应的node就从队列中脱离，然后重新插入到队列头部，这样最近被访问的node就被移到队列开头，冷的对象则逐渐推向队尾，一个最基本的LRU思想(当然这样的处理存在公认的问题，即不能准确判断一个文件的冷热程度，在队尾的也不一定就是冷的)。而这个cache manager进程就是从队尾开始，查看是否有失效的文件。很显然，如果队尾的节点没有失效，那么整个队列上也就都没有问题。后面nginx就会设置定时器，过一段时间再来做检查。但是如果此时缓存满了，又没有文件失效，那么就要做lru来删一些冷的文件，这个也是没有办法的事情。在删的时候，也是从lru队列的尾部开始，如果当前文件已经没有请求在使用，那么就可以直接删了。如果还有请求在用，那么它就从后往前找下一个可以删的对象。一般它尝试找20次，如果找20次还没有找到一个可删的目标，那就先歇一会再处理。删到什么程度结束呢？一般删掉到磁盘使用量低于配额就可。当然你可以指定一些标准，例如删掉总量的10%等等。还有一点要说明一下：因为每个缓存对象都要对应一个内存结构，这些空间来自共享内存，在内存不足的时候分配可能会失败，那么这时也需要采取一些挽救措施。nginx的处理就是去做lru，虽然lru是来删文件，显然在删的同时，内存中的控制结构也会被释放，这样就可以起到缓解内存紧张的局面。
这块的东西基本就是这样，剩下的就是一些代码细节，有兴趣的朋友可以去翻看nginx代码。
来看第二个问题：一个缓存服务器异常停机或者要进行维护关机了，当机器再次启动缓存服务的时候，会遇到一个令人难以接受的状况。因为你之前的机器跑了很久，已经缓存了成千上万的文件，体积高达数百G甚至上T。如果现在这个机器要提供服务，那么会由于之前的缓存不可用(其实是不可见)，而导致大量的回源请求。要是把客户源站给打死了，就等着客户找你算账吧。
所以关键的问题，就是如何重建之前的缓存。其实文件都还在磁盘上，缺失的只是内存中的控制信息，没有这些信息就不能打通整个缓存系统跟实际磁盘文件的联系。在nginx里面，重建缓存这项艰巨的任务由一个叫“cache loader process”的进程来完成，同样的通过ps指令就可以看到。其实原理很简单，我们先看以前文章中的那个例子：
/cache/0/8d/8ef9229f02c5672c747dc7a324d658d0
cache loader process在执行时会去缓存目录下遍历所有的子目录和文件，对于文件需要做一些额外的验证，判读该文件是否符合一个正常缓存文件的特征，例如文件名是否是32字符(例子中的hash串表示的文件名)，文件最少长度(因为在nginx里面，缓存文件开头总是含有特定大小的控制信息)等。对于nginx认为的非合法文件，nginx将会予以删除。假设nginx发现了一个合法的文件，那么就需要将它注册到缓存系统中，具体的动作包括申请结构，加到rbtree和lru队列中等。特别是在插入rbtree的时候，文件名正是原来缓存中用来插入和查找的关键字，所以很方便的就可以完成插入。当cache loader process完成了重建缓存的任务后，就可以exit，因为它已经没有什么存在的价值了。
这里还有一个有意思的地方：如果文件比较多，重建过程可能会很慢，为此nginx使用了三个个变量来控制速度，loader_threshold，loader_files和loader_sleep。如果loader进程持续工作时间达到loader_threshold，那么就去歇一会。如果重建的文件数量到达loader_files，作为奖励，也可以去歇一会，歇息的时间为loader_sleep。这三个变量都是可以通过配置指定的。
这里有个重要的事情要强调一下:
在loader进程重建缓存的时候，其实nginx的worker也在同时提供服务，那么这个loader是否是会影响正常的服务？其实这个loader进程与普通的worker都是由master进程通过fork派生的，所以两者都可以看到共享内存中的一切，由于整个系统的缓存信息都在共享内存中，两者对共享资源的访问就会存在互斥。loader进程中，将一个对象插入到缓存的过程跟worker中是一样的，都是先抢锁，然后查找，没有找到再插入。所以一个对象到底是由谁插入到缓存系统中，完全靠loader和worker公平竞争锁。而那个竞争中的失败者，当它后面拿到锁的时候，就发现让人捷足先登了，那么就跳过插入的工作了。反过来也是一样。
总的看来，nginx在很多机制上处理都非常的简单明了，并且继承了一直以来的高效。虽然站在一个专业cache的角度看，它很多功能还不完善，但是依然有很多值得我们参考和借鉴的东西。如果你想设计和实现一个自己cache，又对功能要求不是那么高，那去参考nginx的这个模型倒是一个不错的选择。
关于cache的设计，还有一个重要的机制就是过期控制。这个至关重要，大家如果对cache有个大概的认识，稍微想一下就能体会它的重要性。这块后面再拿出篇幅来讨论吧。










